# -*- coding: utf-8 -*-
"""VideoMae.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IyXdseGe4ort4FhIKOF9-gWWIQWlGW-N
"""

import os
import zipfile
import shutil
# from sklearn.model_selection import train_test_split

# data set
# Path to your dataset directory
dataset_dir = 'C:\\Users\\UIET\\Desktop\\VideoMae\\PUMAVE'
# unzip_dir = 'C:\\Users\\acer\\Desktop\\PUMAVE\\dataset'
train_dir = 'C:\\Users\\UIET\\Desktop\\VideoMae\\PUMAVE\\train'
test_dir = 'C:\\Users\\UIET\\Desktop\\VideoMae\\PUMAVE\\test'

# Create directories for unzipped data, train, and test splits
# os.makedirs(unzip_dir, exist_ok=True)
# os.makedirs(train_dir, exist_ok=True)
# os.makedirs(test_dir, exist_ok=True)

# Step 1: Unzip the files......................................DONE
# for zip_file in os.listdir(dataset_dir):
#     if zip_file.endswith('.zip'):
#         label = zip_file.split('-')[0]  # Extract the emotion label
#         label_dir = os.path.join(unzip_dir, label)
#         os.makedirs(label_dir, exist_ok=True)

#         with zipfile.ZipFile(os.path.join(dataset_dir, zip_file), 'r') as zip_ref:
#             zip_ref.extractall(label_dir)

# Step 2: Split into train and test sets
# for label in os.listdir(unzip_dir):
#     files = os.listdir(os.path.join(unzip_dir, label))
#     train_files, test_files = train_test_split(files, test_size=0.2,train_size=0.8, random_state=42)

#     # Create label directories in train and test folders
#     os.makedirs(os.path.join(train_dir, label), exist_ok=True)
#     os.makedirs(os.path.join(test_dir, label), exist_ok=True)

#     # Move train files
#     for file in train_files:
#         shutil.move(os.path.join(unzip_dir, label, file), os.path.join(train_dir, label, file))

#     # Move test files
#     for file in test_files:
#         shutil.move(os.path.join(unzip_dir, label, file), os.path.join(test_dir, label, file))

# # Step 3: Clean up the unzipped directory
# shutil.rmtree(unzip_dir)

# print("Dataset unzipped and split into train/test successfully.")

#LABELS *************************************************************************************************************************************


def get_video_paths_and_labels(base_dir):
    video_paths = []
    labels = []
    for root, _, files in os.walk(base_dir):
        for file in files:
            if file.endswith(('.mp4', '.avi', '.mpg')):
                video_paths.append(os.path.join(root, file))
                labels.append(root.split(os.sep)[-1])
    return video_paths, labels

train_video_paths, train_labels = get_video_paths_and_labels(train_dir)
val_video_paths, val_labels = get_video_paths_and_labels(test_dir)

class_labels = sorted({os.path.basename(os.path.dirname(path)) for path in train_video_paths})

# Create mappings from labels to IDs and vice versa
label2id = {label: i for i, label in enumerate(class_labels)}
id2label = {i: label for label, i in label2id.items()}

print(f"Unique classes: {list(label2id.keys())}.")

import torch
from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification

model_ckpt = "MCG-NJU/videomae-base"
image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
model = VideoMAEForVideoClassification.from_pretrained(
    model_ckpt,
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
)

model.load_state_dict(torch.load('model2Facial68.pth'))

# !pip install torchvision ffmpeg-python ########################################################################################################################

mean = image_processor.image_mean
std = image_processor.image_std
if "shortest_edge" in image_processor.size:
    height = width = image_processor.size["shortest_edge"]
else:
    height = image_processor.size["height"]
    width = image_processor.size["width"]
resize_to = (height, width)
print(resize_to)

num_frames_to_sample = model.config.num_frames
sample_rate = 4
fps = 30
clip_duration = num_frames_to_sample * sample_rate / fps

# num_frames_to_sample, clip_duration

# !pip install imageio imageio-ffmpeg##############################################################################################################################

import os
import numpy as np
import imageio
from torch.utils.data import Dataset, DataLoader
from torchvision.transforms import Lambda
from transformers import VideoMAEImageProcessor

# Initialize Hugging Face VideoMAEImageProcessor
model_ckpt = "MCG-NJU/videomae-base"
image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)
mean = image_processor.image_mean
std = image_processor.image_std
resize_to = image_processor.size["shortest_edge"] if "shortest_edge" in image_processor.size else image_processor.size["height"]

# Updated Custom Video Dataset with Hugging Face preprocessing


import cv2
import dlib
from mtcnn import MTCNN
import torch
import numpy as np
import imageio
from torch.utils.data import Dataset

# Initialize the MTCNN face detector and dlib's facial landmark predictor  ### torch.compile()
detector = MTCNN()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks (1).dat')  # Download this model file from dlib's website

class CustomVideoDataset(Dataset):
    def __init__(self, video_paths, labels, processor, num_frames_to_sample=16, sample_rate=4):
        self.video_paths = video_paths
        self.labels = labels
        self.processor = processor
        self.num_frames_to_sample = num_frames_to_sample
        self.sample_rate = sample_rate

    def __len__(self):
        return len(self.video_paths)

    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        label = self.labels[idx]

        # Load and sample video frames using imageio
        video_reader = imageio.get_reader(video_path, 'ffmpeg')
        frames = []
        for i, frame in enumerate(video_reader):
            if i % self.sample_rate == 0:  # Sample frames based on the sample rate
                # Convert frame to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                



                # Detect face and crop
                faces = detector.detect_faces(frame_rgb)
                if len(faces) > 0:
                    x, y, width, height = faces[0]['box']  # Get the bounding box for the first face
                    cropped_face = frame_rgb[y:y+height, x:x+width]
                    
                    # Convert cropped face to grayscale for dlib
                    gray_face = cv2.cvtColor(cropped_face, cv2.COLOR_RGB2GRAY)
                    
                    # Detect facial landmarks
                    rect = dlib.rectangle(0, 0, cropped_face.shape[1], cropped_face.shape[0])
                    landmarks = predictor(gray_face, rect)
                    
                    # Draw landmarks on the cropped face
                    for n in range(0, 68):
                        x_lm = landmarks.part(n).x
                        y_lm = landmarks.part(n).y
                        cv2.circle(cropped_face, (x_lm, y_lm), 2, (255, 0, 0), -1)
                    
                    # Add the face with landmarks to the frames list
                    cropped_face = cv2.resize(cropped_face, (224, 224))
  
                    frames.append(cropped_face)
                else:
                    frame_rgb = cv2.resize(frame_rgb, (224, 224))
                    frames.append(frame_rgb)  # If no face detected, use the original frame

            if len(frames) >= self.num_frames_to_sample:
                break

        video_reader.close()

        # Convert frames to torch tensor and apply VideoMAE preprocessing
        video = torch.tensor(np.stack(frames), dtype=torch.float32).permute(0, 3, 1, 2)  # (T, C, H, W)
        video = self.processor(list(video), return_tensors="pt")["pixel_values"].squeeze()  # Preprocessing with Hugging Face processor
        
        # Convert label to tensor
        label = torch.tensor(label2id[label], dtype=torch.long)
        
        return {"pixel_values": video, "label": label}


# Example usage to create DataLoaders
train_dataset = CustomVideoDataset(train_video_paths, train_labels, processor=image_processor)
val_dataset = CustomVideoDataset(val_video_paths, val_labels, processor=image_processor)

def pad_videos(batch):
    videos = [item['pixel_values'] for item in batch]
    labels = torch.tensor([item['label'] for item in batch])

    # Find the length of the longest video
    # max_len = max([v.size(0) for v in videos])
    max_len=16
    # Pad videos to the max length
    padded_videos = []
    for v in videos:
        pad_size = max_len - v.size(0)
        padded_videos.append(torch.nn.functional.pad(v, (0, 0, 0, 0, 0, 0, 0, pad_size)))

    padded_videos = torch.stack(padded_videos)

    return {"pixel_values": padded_videos, "label": labels}

train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=pad_videos)
val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=pad_videos)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader

optimizer = optim.AdamW(model.parameters(), lr=5e-5)
criterion = nn.CrossEntropyLoss()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)

from tqdm import tqdm

epochs = 4
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")

    # Training phase
    model.train()
    running_train_loss = 0.0
    correct_train = 0
    total_train = 0

    for batch in tqdm(train_loader, desc="Training", leave=False):
        inputs = batch['pixel_values'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.step()

        # Accumulate loss and accuracy
        running_train_loss += loss.item()
        _, predicted = torch.max(outputs.logits, 1)
        total_train += labels.size(0)
        correct_train += (predicted == labels).sum().item()

    train_loss = running_train_loss / len(train_loader)
    train_acc = 100 * correct_train / total_train

    print(f"Training Loss: {train_loss:.4f}, Training Accuracy: {train_acc:.2f}%")

    # Validation phase
    model.eval()
    running_val_loss = 0.0
    correct_val = 0
    total_val = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc="Validating", leave=False):
            inputs = batch['pixel_values'].to(device)
            labels = batch['label'].to(device)

            outputs = model(inputs)
            loss = criterion(outputs.logits, labels)

            # Accumulate loss and accuracy
            running_val_loss += loss.item()
            _, predicted = torch.max(outputs.logits, 1)
            total_val += labels.size(0)
            correct_val += (predicted == labels).sum().item()

    val_loss = running_val_loss / len(val_loader)
    val_acc = 100 * correct_val / total_val

    print(f"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%")

    print("-" * 50)

print("Training complete!")
torch.save(model.state_dict(), 'model2Facial68.pth')
print("Model saved successfully")
